# api.py è¯¦ç»†åˆ†æ

## ğŸ“‹ æ–‡ä»¶æ¦‚è§ˆ

**æ–‡ä»¶å**: api.py  
**ä»£ç è¡Œæ•°**: 110 è¡Œ  
**èŒè´£**: æš´éœ² REST API ç«¯ç‚¹ï¼Œå¤„ç† HTTP è¯·æ±‚/å“åº”  
**æ ¸å¿ƒç±»**: 8 ä¸ª Pydantic æ¨¡å‹ + 1 ä¸ªå·¥å‚å‡½æ•° + 4 ä¸ªè·¯ç”±å¤„ç†å™¨  

---

## ğŸ—ï¸ ä»£ç ç»“æ„

### æ–‡ä»¶ç»„ç»‡ (File Organization)

```
api.py
â”œâ”€ å¯¼å…¥éƒ¨åˆ† (Lines 1-11)
â”‚  â”œâ”€ æ ‡å‡†åº“
â”‚  â”œâ”€ ç¬¬ä¸‰æ–¹åº“ (fastapi, pydantic)
â”‚  â””â”€ æœ¬åœ°æ¨¡å— (config, embeddings)
â”‚
â”œâ”€ æ•°æ®æ¨¡å‹éƒ¨åˆ† (Lines 13-43)
â”‚  â”œâ”€ è¯·æ±‚æ¨¡å‹ (3 ä¸ª)
â”‚  â”‚  â”œâ”€ QueryRequest
â”‚  â”‚  â”œâ”€ DocumentsRequest
â”‚  â”‚  â””â”€ ChatRequest
â”‚  â”œâ”€ å“åº”æ¨¡å‹ (4 ä¸ª)
â”‚  â”‚  â”œâ”€ EmbeddingResponse
â”‚  â”‚  â”œâ”€ EmbeddingsResponse
â”‚  â”‚  â”œâ”€ ChatResponse
â”‚  â”‚  â””â”€ HealthResponse
â”‚  â””â”€ è¾…åŠ©å‡½æ•°
â”‚
â”œâ”€ åº”ç”¨å·¥å‚éƒ¨åˆ† (Lines 45-108)
â”‚  â”œâ”€ create_app() å‡½æ•°
â”‚  â”œâ”€ é…ç½®åŠ è½½
â”‚  â”œâ”€ æ¨¡å‹åˆå§‹åŒ–
â”‚  â”œâ”€ é”™è¯¯å¤„ç†
â”‚  â””â”€ è·¯ç”±æ³¨å†Œ
â”‚
â””â”€ åº”ç”¨å®ä¾‹éƒ¨åˆ† (Line 111)
   â””â”€ app = create_app()
```

---

## ğŸ“¦ Pydantic æ•°æ®æ¨¡å‹åˆ†æ

### 1. è¯·æ±‚æ¨¡å‹ (Request Models)

#### QueryRequest (Line 15-16)
```python
class QueryRequest(BaseModel):
    text: str
```

**ç›®çš„**: å°è£…å•ä¸ªæ–‡æœ¬æŸ¥è¯¢è¯·æ±‚  
**å­—æ®µ**:
- `text: str` - å¾…åµŒå…¥çš„æ–‡æœ¬ï¼ˆå¿…éœ€ï¼‰

**ä½¿ç”¨åœºæ™¯**: POST /embed/query  
**éªŒè¯è§„åˆ™**: FastAPI è‡ªåŠ¨éªŒè¯
- å¿…é¡»æ˜¯å­—ç¬¦ä¸²
- ä¸èƒ½ä¸ºç©ºï¼ˆé»˜è®¤ï¼‰
- è‡ªåŠ¨ç”Ÿæˆ OpenAPI schema

**ç¤ºä¾‹**:
```json
{
  "text": "Machine learning is awesome"
}
```

---

#### DocumentsRequest (Line 19-20)
```python
class DocumentsRequest(BaseModel):
    texts: List[str]
```

**ç›®çš„**: å°è£…å¤šæ–‡æœ¬æ‰¹é‡åµŒå…¥è¯·æ±‚  
**å­—æ®µ**:
- `texts: List[str]` - å¾…åµŒå…¥çš„æ–‡æœ¬åˆ—è¡¨ï¼ˆå¿…éœ€ï¼‰

**ä½¿ç”¨åœºæ™¯**: POST /embed/documents  
**éªŒè¯è§„åˆ™**:
- å¿…é¡»æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
- æ¯ä¸ªå­—ç¬¦ä¸²éªŒè¯åŒ QueryRequest

**ç¤ºä¾‹**:
```json
{
  "texts": [
    "Document 1",
    "Document 2",
    "Document 3"
  ]
}
```

**æ€§èƒ½è€ƒè™‘**:
- æ— é™åˆ¶çš„åˆ—è¡¨é•¿åº¦ï¼ˆåº”æ·»åŠ æœ€å¤§å€¼ï¼‰
- æ‰¹é‡å¤§å°ç”± LangChain å†…éƒ¨å¤„ç†

---

#### ChatRequest (Line 23-24)
```python
class ChatRequest(BaseModel):
    message: str
```

**ç›®çš„**: å°è£…èŠå¤©æ¶ˆæ¯è¯·æ±‚  
**å­—æ®µ**:
- `message: str` - ç”¨æˆ·æ¶ˆæ¯ï¼ˆå¿…éœ€ï¼‰

**ä½¿ç”¨åœºæ™¯**: POST /chat  
**éªŒè¯è§„åˆ™**: åŒ QueryRequest

**ç¤ºä¾‹**:
```json
{
  "message": "What is artificial intelligence?"
}
```

---

### 2. å“åº”æ¨¡å‹ (Response Models)

#### EmbeddingResponse (Line 27-28)
```python
class EmbeddingResponse(BaseModel):
    embedding: List[float]
```

**ç›®çš„**: è¿”å›å•ä¸ªåµŒå…¥å‘é‡  
**å­—æ®µ**:
- `embedding: List[float]` - å‘é‡æ•°æ®ï¼ˆé€šå¸¸ 1024 ç»´ï¼‰

**ä½¿ç”¨åœºæ™¯**: POST /embed/query çš„å“åº”  
**æ•°æ®å¤§å°**: ~4KB (1024 floats Ã— 4 bytes)

**ç¤ºä¾‹**:
```json
{
  "embedding": [0.123, -0.456, ..., 0.789]  // 1024 ä¸ªæµ®ç‚¹æ•°
}
```

---

#### EmbeddingsResponse (Line 31-32)
```python
class EmbeddingsResponse(BaseModel):
    embeddings: List[List[float]]
```

**ç›®çš„**: è¿”å›å¤šä¸ªåµŒå…¥å‘é‡  
**å­—æ®µ**:
- `embeddings: List[List[float]]` - å‘é‡åˆ—è¡¨

**ä½¿ç”¨åœºæ™¯**: POST /embed/documents çš„å“åº”  
**æ•°æ®å¤§å°**: N Ã— 4KB (N = æ–‡æœ¬æ•°)

**ç¤ºä¾‹**:
```json
{
  "embeddings": [
    [0.123, -0.456, ..., 0.789],
    [0.234, -0.567, ..., 0.890],
    [0.345, -0.678, ..., 0.901]
  ]
}
```

---

#### ChatResponse (Line 35-36)
```python
class ChatResponse(BaseModel):
    response: str
```

**ç›®çš„**: è¿”å›èŠå¤©å›å¤  
**å­—æ®µ**:
- `response: str` - æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬

**ä½¿ç”¨åœºæ™¯**: POST /chat çš„å“åº”  
**å¤§å°**: é€šå¸¸ 100-2000 å­—ç¬¦

**ç¤ºä¾‹**:
```json
{
  "response": "Artificial intelligence is the simulation of human intelligence by computer systems..."
}
```

---

#### HealthResponse (Line 39-43)
```python
class HealthResponse(BaseModel):
    status: str
    provider: str
    embed_model: str
    llm_model: str
```

**ç›®çš„**: è¿”å›æœåŠ¡å¥åº·çŠ¶æ€å’Œé…ç½®ä¿¡æ¯  
**å­—æ®µ**:
- `status: str` - "ok" æˆ– "error"
- `provider: str` - "ollama" æˆ– "openai-compatible"
- `embed_model: str` - åµŒå…¥æ¨¡å‹åç§°
- `llm_model: str` - LLM æ¨¡å‹åç§°

**ä½¿ç”¨åœºæ™¯**: GET /health çš„å“åº”  
**ç”¨é€”**: 
- è´Ÿè½½å‡è¡¡å™¨å¥åº·æ£€æŸ¥
- ç›‘æ§ç³»ç»ŸçŠ¶æ€
- éªŒè¯é…ç½®

**ç¤ºä¾‹**:
```json
{
  "status": "ok",
  "provider": "ollama",
  "embed_model": "mxbai-embed-large",
  "llm_model": "qwen2.5:3b"
}
```

---

## ğŸ­ å·¥å‚å‡½æ•°åˆ†æ

### create_app() å‡½æ•° (Lines 46-110)

#### å‡½æ•°ç­¾å
```python
def create_app() -> FastAPI:
    """Create FastAPI application."""
```

**è¿”å›ç±»å‹**: FastAPI åº”ç”¨å®ä¾‹  
**èŒè´£**:
1. åŠ è½½é…ç½®
2. åˆå§‹åŒ–æ¨¡å‹
3. æ³¨å†Œè·¯ç”±
4. è¿”å›å®Œæ•´çš„åº”ç”¨

#### ç¬¬ä¸€æ­¥: é…ç½®åŠ è½½ (Lines 47-48)

```python
settings = Settings.from_env()
app = FastAPI(title="Embedding Service API")
```

**ä½œç”¨**:
- ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
- åˆ›å»º FastAPI åº”ç”¨å®ä¾‹

**é…ç½®ç¤ºä¾‹**:
```python
# Settings å®ä¾‹åŒ…å«:
settings.provider              # "ollama" æˆ– "openai-compatible"
settings.embed_model           # "mxbai-embed-large"
settings.llm_model            # "qwen2.5:3b"
settings.ollama_base_url      # "http://localhost:11434"
settings.openai_base_url      # None æˆ– "https://api.openai.com/v1"
settings.openai_api_key       # None æˆ– "sk-..."
```

#### ç¬¬äºŒæ­¥: æ¨¡å‹åˆå§‹åŒ– (Lines 50-60)

```python
try:
    embeddings = build_embeddings(
        settings.provider,
        settings.embed_model,
        base_url=settings.ollama_base_url if settings.provider == "ollama" else settings.openai_base_url,
        api_key=settings.openai_api_key,
    )
    chat_model = build_chat_model(
        settings.provider,
        settings.llm_model,
        base_url=settings.ollama_base_url if settings.provider == "ollama" else settings.openai_base_url,
        api_key=settings.openai_api_key,
    )
except Exception as e:
    raise RuntimeError(f"Failed to initialize models: {e}")
```

**å…³é”®é€»è¾‘**:

| æä¾›å•† | base_url | api_key |
|--------|----------|---------|
| `ollama` | `ollama_base_url` | ä¸éœ€è¦ |
| `openai-compatible` | `openai_base_url` | `openai_api_key` |

**é”™è¯¯å¤„ç†**:
- âœ… æ•è·æ¨¡å‹åˆå§‹åŒ–å¼‚å¸¸
- âœ… åŒ…è£…ä¸º RuntimeError
- âŒ ä¸æä¾›è¯¦ç»†çš„é”™è¯¯æ¶ˆæ¯åˆ†ç±»

**å¯èƒ½çš„å¼‚å¸¸**:
- `ConnectionError` - æœåŠ¡ä¸å¯ç”¨
- `AuthenticationError` - API å¯†é’¥æ— æ•ˆ
- `ModelNotFoundError` - æ¨¡å‹æœªæ‰¾åˆ°

#### ç¬¬ä¸‰æ­¥: è·¯ç”±æ³¨å†Œ (Lines 62-107)

4 ä¸ªè·¯ç”±å¤„ç†å™¨è¢«æ³¨å†Œåˆ°åº”ç”¨ã€‚

---

## ğŸ›£ï¸ è·¯ç”±å¤„ç†å™¨åˆ†æ

### 1. GET /health ç«¯ç‚¹ (Lines 62-71)

```python
@app.get("/health", response_model=HealthResponse)
def health() -> HealthResponse:
    """Health check endpoint."""
    return HealthResponse(
        status="ok",
        provider=settings.provider,
        embed_model=settings.embed_model,
        llm_model=settings.llm_model,
    )
```

**URL**: `GET /health`  
**å“åº”æ¨¡å‹**: `HealthResponse`  
**HTTP çŠ¶æ€ç **: 200 (æˆåŠŸ)

**ç”¨é€”**:
- è´Ÿè½½å‡è¡¡å™¨æ¢é’ˆ
- ç›‘æ§ç³»ç»Ÿæ£€æŸ¥
- éªŒè¯é…ç½®

**æµç¨‹**:
```
GET /health
  â†“
health()
  â†“
è¯»å– settings å±æ€§
  â†“
æ„é€  HealthResponse
  â†“
FastAPI åºåˆ—åŒ–ä¸º JSON
  â†“
HTTP 200 OK
```

**å“åº”ç¤ºä¾‹**:
```json
HTTP/1.1 200 OK
Content-Type: application/json

{
  "status": "ok",
  "provider": "ollama",
  "embed_model": "mxbai-embed-large",
  "llm_model": "qwen2.5:3b"
}
```

**æ€§èƒ½**: æå¿« (< 1ms)  
**å‰¯ä½œç”¨**: æ—   
**å¯é æ€§**: å¾ˆé«˜ï¼ˆæ— å¤–éƒ¨ä¾èµ–ï¼‰

---

### 2. POST /embed/query ç«¯ç‚¹ (Lines 73-80)

```python
@app.post("/embed/query", response_model=EmbeddingResponse)
def embed_query(request: QueryRequest) -> EmbeddingResponse:
    """Embed a single query text."""
    try:
        vector = embeddings.embed_query(request.text)
        return EmbeddingResponse(embedding=vector)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

**URL**: `POST /embed/query`  
**è¯·æ±‚ä½“**: `QueryRequest { text: str }`  
**å“åº”ä½“**: `EmbeddingResponse { embedding: List[float] }`  

**æµç¨‹**:
```
POST /embed/query
â”œâ”€ Content-Type: application/json
â”œâ”€ Body: { "text": "..." }
  â†“
FastAPI è§£æè¯·æ±‚
  â†“
Pydantic éªŒè¯ QueryRequest
  â”œâ”€ æ£€æŸ¥ text æ˜¯å­—ç¬¦ä¸²
  â”œâ”€ æ£€æŸ¥ text ä¸ä¸ºç©ºï¼ˆå¯é€‰ï¼‰
  â””â”€ ç»‘å®šåˆ° request å‚æ•°
  â†“
embed_query(request)
  â†“
embeddings.embed_query(request.text)
  â”œâ”€ LangChain æ¡†æ¶
  â”œâ”€ è°ƒç”¨ Ollama / OpenAI
  â””â”€ è¿”å›å‘é‡ [float, float, ...]
  â†“
å¼‚å¸¸å¤„ç†:
â”œâ”€ æˆåŠŸ: è¿”å› EmbeddingResponse
â””â”€ å¤±è´¥: æ•è· â†’ HTTPException(500)
  â†“
FastAPI åºåˆ—åŒ–å“åº”
  â†“
HTTP å“åº” (200 æˆ– 500)
```

**è¯·æ±‚ç¤ºä¾‹**:
```bash
curl -X POST http://localhost:8000/embed/query \
  -H "Content-Type: application/json" \
  -d '{"text": "Hello world"}'
```

**æˆåŠŸå“åº”** (200 OK):
```json
{
  "embedding": [0.123, -0.456, ..., 0.789]
}
```

**å¤±è´¥å“åº”** (500 Internal Server Error):
```json
{
  "detail": "Model not found: mxbai-embed-large"
}
```

**æ€§èƒ½**:
- å»¶è¿Ÿ: ~100-200ms
- å—é™äº Ollama / OpenAI å“åº”æ—¶é—´

**é”™è¯¯åœºæ™¯**:
- âŒ Ollama æœåŠ¡ç¦»çº¿
- âŒ OpenAI API é…é¢ç”¨å°½
- âŒ ç½‘ç»œè¶…æ—¶
- âŒ æ¨¡å‹æœªä¸‹è½½

---

### 3. POST /embed/documents ç«¯ç‚¹ (Lines 82-89)

```python
@app.post("/embed/documents", response_model=EmbeddingsResponse)
def embed_documents(request: DocumentsRequest) -> EmbeddingsResponse:
    """Embed multiple documents."""
    try:
        vectors = embeddings.embed_documents(request.texts)
        return EmbeddingsResponse(embeddings=vectors)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

**URL**: `POST /embed/documents`  
**è¯·æ±‚ä½“**: `DocumentsRequest { texts: List[str] }`  
**å“åº”ä½“**: `EmbeddingsResponse { embeddings: List[List[float]] }`  

**æµç¨‹**: åŒ `/embed/query`ï¼Œä½†æ‰¹é‡å¤„ç†

**è¯·æ±‚ç¤ºä¾‹**:
```bash
curl -X POST http://localhost:8000/embed/documents \
  -H "Content-Type: application/json" \
  -d '{
    "texts": [
      "Document 1",
      "Document 2",
      "Document 3"
    ]
  }'
```

**æˆåŠŸå“åº”** (200 OK):
```json
{
  "embeddings": [
    [0.123, -0.456, ..., 0.789],
    [0.234, -0.567, ..., 0.890],
    [0.345, -0.678, ..., 0.901]
  ]
}
```

**æ€§èƒ½**:
- å»¶è¿Ÿ: ~500ms (100 æ–‡æœ¬) åˆ° ~2s (1000 æ–‡æœ¬)
- LangChain å†…éƒ¨è¿›è¡Œæ‰¹å¤„ç†

**æ‰¹å¤„ç†ä¼˜åŒ–**:
```
texts: [100 ä¸ªæ–‡æœ¬]
  â†“
LangChain (å†…éƒ¨)
  â”œâ”€ åˆ†æˆ batch_size=20 çš„æ‰¹æ¬¡
  â”œâ”€ ç¬¬1æ‰¹ â†’ API è°ƒç”¨ 1
  â”œâ”€ ç¬¬2æ‰¹ â†’ API è°ƒç”¨ 2
  â”œâ”€ ...
  â””â”€ ç¬¬5æ‰¹ â†’ API è°ƒç”¨ 5
  â†“
åˆå¹¶æ‰€æœ‰å‘é‡
  â†“
è¿”å› [List[float]] Ã— 100
```

**æ€§èƒ½è€ƒè™‘** âš ï¸:
- æ— æœ€å¤§é•¿åº¦é™åˆ¶ï¼ˆåº”æ·»åŠ  `max_items` éªŒè¯ï¼‰
- å¤§æ‰¹é‡è¯·æ±‚å¯èƒ½å¯¼è‡´è¶…æ—¶

---

### 4. POST /chat ç«¯ç‚¹ (Lines 91-107)

```python
@app.post("/chat", response_model=ChatResponse)
def chat(request: ChatRequest) -> ChatResponse:
    """Chat completion endpoint."""
    try:
        from langchain_core.messages import HumanMessage
        
        response = chat_model.invoke([HumanMessage(content=request.message)])
        return ChatResponse(response=response.content)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

**URL**: `POST /chat`  
**è¯·æ±‚ä½“**: `ChatRequest { message: str }`  
**å“åº”ä½“**: `ChatResponse { response: str }`  

**æµç¨‹**:
```
POST /chat
â”œâ”€ Content-Type: application/json
â”œâ”€ Body: { "message": "..." }
  â†“
FastAPI è§£æ + éªŒè¯
  â†“
chat(request)
  â†“
from langchain_core.messages import HumanMessage
  â””â”€ åŠ¨æ€å¯¼å…¥ï¼ˆæ€§èƒ½å½±å“ï¼‰
  â†“
HumanMessage(content=request.message)
  â”œâ”€ æ„é€  LangChain æ¶ˆæ¯å¯¹è±¡
  â””â”€ æ ¼å¼: {"type": "human", "content": "..."}
  â†“
chat_model.invoke([msg])
  â”œâ”€ è°ƒç”¨ Ollama / OpenAI
  â”œâ”€ æ‰§è¡Œ LLM æ¨ç†
  â””â”€ è¿”å› AIMessage
  â†“
response.content
  â””â”€ æå–æ–‡æœ¬éƒ¨åˆ†
  â†“
ChatResponse(response=...)
  â†“
åºåˆ—åŒ–ä¸º JSON
  â†“
HTTP 200 OK
```

**è¯·æ±‚ç¤ºä¾‹**:
```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "What is machine learning?"}'
```

**æˆåŠŸå“åº”** (200 OK):
```json
{
  "response": "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience..."
}
```

**æ€§èƒ½**:
- å»¶è¿Ÿ: 2-10 ç§’ (Ollama qwen3b)
- å»¶è¿Ÿ: 1-3 ç§’ (OpenAI gpt-4)

**åŠ¨æ€å¯¼å…¥é—®é¢˜** âš ï¸:
```python
# é—®é¢˜: æ¯æ¬¡è¯·æ±‚éƒ½å¯¼å…¥
from langchain_core.messages import HumanMessage

# æ”¹è¿›: åœ¨æ–‡ä»¶å¼€å¤´å¯¼å…¥
# from langchain_core.messages import HumanMessage
```

---

## ğŸ”’ é”™è¯¯å¤„ç†åˆ†æ

### ç°æœ‰é”™è¯¯å¤„ç†

```python
try:
    # æ‰§è¡Œ LLM æ“ä½œ
    vector = embeddings.embed_query(request.text)
except Exception as e:
    # æ•è·æ‰€æœ‰å¼‚å¸¸
    raise HTTPException(status_code=500, detail=str(e))
```

**é—®é¢˜**:
1. âŒ è¿‡äºå®½æ³›ï¼ˆæ•è· Exceptionï¼‰
2. âŒ æ— é”™è¯¯åˆ†ç±»
3. âŒ æ— æ—¥å¿—è®°å½•
4. âŒ æ— é‡è¯•æœºåˆ¶
5. âš ï¸ é”™è¯¯æ¶ˆæ¯å¯èƒ½æš´éœ²å†…éƒ¨ç»†èŠ‚

### æ”¹è¿›å»ºè®®

```python
# 1. å…·ä½“å¼‚å¸¸å¤„ç†
from langchain.errors import LLMError, APIConnectionError

try:
    vector = embeddings.embed_query(request.text)
except APIConnectionError as e:
    logger.error(f"Connection failed: {e}")
    raise HTTPException(status_code=503, detail="Service unavailable")
except LLMError as e:
    logger.error(f"LLM error: {e}")
    raise HTTPException(status_code=500, detail="Model error")
except Exception as e:
    logger.exception(f"Unexpected error: {e}")
    raise HTTPException(status_code=500, detail="Internal error")

# 2. æ·»åŠ æ—¥å¿—
import logging
logger = logging.getLogger(__name__)

# 3. é‡è¯•æœºåˆ¶
from tenacity import retry, stop_after_attempt

@retry(stop=stop_after_attempt(3))
def embed_query_with_retry(text: str):
    return embeddings.embed_query(text)
```

---

## ğŸ“Š API ç«¯ç‚¹æ€»ç»“è¡¨

| ç«¯ç‚¹ | æ–¹æ³• | è¯·æ±‚ | å“åº” | å»¶è¿Ÿ | ç”¨é€” |
|------|------|------|------|------|------|
| `/health` | GET | - | HealthResponse | <1ms | å¥åº·æ£€æŸ¥ |
| `/embed/query` | POST | QueryRequest | EmbeddingResponse | ~100ms | å•æ–‡æœ¬åµŒå…¥ |
| `/embed/documents` | POST | DocumentsRequest | EmbeddingsResponse | ~500ms-2s | æ‰¹é‡åµŒå…¥ |
| `/chat` | POST | ChatRequest | ChatResponse | ~2-10s | èŠå¤©è¡¥å…¨ |

---

## ğŸ¯ å…³é”®ç‰¹ç‚¹

### âœ… ä¼˜ç‚¹

1. **ç®€æ´** - ä»… 110 è¡Œä»£ç 
2. **è‡ªåŠ¨æ–‡æ¡£** - FastAPI è‡ªåŠ¨ç”Ÿæˆ Swagger UI
3. **ç±»å‹å®‰å…¨** - Pydantic è‡ªåŠ¨éªŒè¯
4. **ç»“æ„æ¸…æ™°** - æ•°æ®æ¨¡å‹ + å·¥å‚ + è·¯ç”±åˆ†ç¦»
5. **æ˜“äºæµ‹è¯•** - ä¾èµ–æ³¨å…¥ï¼Œä¾¿äº Mock

### âš ï¸ æ”¹è¿›ç©ºé—´

1. **é”™è¯¯å¤„ç†** - è¿‡äºå®½æ³›ï¼Œéœ€è¦æ›´ç²¾ç»†çš„åˆ†ç±»
2. **æ—¥å¿—ç¼ºå¤±** - æ— ç»“æ„åŒ–æ—¥å¿—è¾“å‡º
3. **æ€§èƒ½ä¼˜åŒ–** - åŠ¨æ€å¯¼å…¥ï¼ˆ/chat ç«¯ç‚¹ï¼‰
4. **éªŒè¯ä¸è¶³** - æ— è¾“å…¥å¤§å°é™åˆ¶
5. **ç›‘æ§ç¼ºå¤±** - æ— æ€§èƒ½æŒ‡æ ‡æ”¶é›†
6. **è¶…æ—¶å¤„ç†** - æ— è¶…æ—¶è®¾ç½®
7. **é€Ÿç‡é™åˆ¶** - æ— é™æµæ§åˆ¶

---

## ğŸ”§ ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1: æœ¬åœ°å¼€å‘

```bash
# å¯åŠ¨ Ollama
ollama serve

# å¯åŠ¨ API
PROVIDER=ollama EMBED_MODEL=mxbai-embed-large \
uvicorn embedding_service.api:app --reload

# æµ‹è¯•
curl http://localhost:8000/health
```

### åœºæ™¯ 2: Docker éƒ¨ç½²

```bash
# Dockerfile
FROM python:3.11
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY embedding_service ./embedding_service
ENV PROVIDER=ollama
ENV OLLAMA_BASE_URL=http://ollama:11434
CMD ["uvicorn", "embedding_service.api:app", "--host", "0.0.0.0"]
```

### åœºæ™¯ 3: ç”Ÿäº§éƒ¨ç½²

```yaml
# kubernetes deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: embedding-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: embedding-service
  template:
    metadata:
      labels:
        app: embedding-service
    spec:
      containers:
      - name: api
        image: embedding-service:latest
        ports:
        - containerPort: 8000
        env:
        - name: PROVIDER
          value: "ollama"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
```

---

## ğŸ“ˆ å¯æ‰©å±•æ€§

### æ·»åŠ æ–°ç«¯ç‚¹

```python
# åœ¨ create_app() ä¸­æ·»åŠ :

class SimilarityRequest(BaseModel):
    vector1: List[float]
    vector2: List[float]

class SimilarityResponse(BaseModel):
    similarity: float

@app.post("/similarity", response_model=SimilarityResponse)
def similarity(request: SimilarityRequest) -> SimilarityResponse:
    """Calculate cosine similarity between two vectors."""
    from sklearn.metrics.pairwise import cosine_similarity
    import numpy as np
    
    v1 = np.array(request.vector1).reshape(1, -1)
    v2 = np.array(request.vector2).reshape(1, -1)
    score = cosine_similarity(v1, v2)[0][0]
    return SimilarityResponse(similarity=float(score))
```

---

## æ€»ç»“

**api.py** æ˜¯ä¸€ä¸ªç²¾ç®€ä½†åŠŸèƒ½å®Œæ•´çš„ REST API å®ç°ï¼Œæä¾›äº† 4 ä¸ªæ ¸å¿ƒç«¯ç‚¹æ¥æ”¯æŒåµŒå…¥å’ŒèŠå¤©æ“ä½œã€‚å®ƒå±•ç¤ºäº†ä»¥ä¸‹æœ€ä½³å®è·µï¼š

âœ… ä½¿ç”¨ Pydantic è¿›è¡Œæ•°æ®éªŒè¯  
âœ… ä½¿ç”¨å·¥å‚æ¨¡å¼åˆ›å»ºåº”ç”¨  
âœ… åˆ†ç¦»æ•°æ®æ¨¡å‹å’Œä¸šåŠ¡é€»è¾‘  
âœ… åŸºäºç±»å‹æç¤ºçš„è‡ªåŠ¨æ–‡æ¡£  

ä½†åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éœ€è¦æ·»åŠ ï¼š
- æ›´ç²¾ç»†çš„é”™è¯¯å¤„ç†
- ç»“æ„åŒ–æ—¥å¿—
- æ€§èƒ½ç›‘æ§
- é€Ÿç‡é™åˆ¶
- è¶…æ—¶è®¾ç½®

