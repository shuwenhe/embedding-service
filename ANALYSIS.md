# Embedding-Service é¡¹ç›®è¯¦ç»†åˆ†æ

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

**é¡¹ç›®åç§°**: Embedding Service  
**é¡¹ç›®ç±»å‹**: å¯å¤ç”¨çš„ AI æ¨¡å‹æœåŠ¡åº“  
**æŠ€æœ¯æ ˆ**: LangChain + FastAPI + Pydantic  
**ä¸»è¦åŠŸèƒ½**: ç»Ÿä¸€çš„æ–‡æœ¬åµŒå…¥å’ŒèŠå¤©æ¨¡å‹æ¥å£ï¼Œæ”¯æŒå¤šä¸ª LLM æä¾›å•†  
**ç‰ˆæœ¬**: 0.1.0  
**æ¥æº**: ä» airport-customer é¡¹ç›®ä¸­æå–  

---

## ğŸ¯ é¡¹ç›®ç›®æ ‡

### æ ¸å¿ƒç›®æ ‡
æä¾›ä¸€ä¸ª**è½»é‡çº§ã€å¯å¤ç”¨çš„ AI æ¨¡å‹æœåŠ¡åº“**ï¼Œç»Ÿä¸€ä¸åŒ LLM æä¾›å•†çš„æ¥å£ï¼Œä¾¿äºï¼š
- å¿«é€Ÿé›†æˆæ–‡æœ¬åµŒå…¥åŠŸèƒ½
- æ”¯æŒèŠå¤©/ç”Ÿæˆä»»åŠ¡
- è½»æ¾åˆ‡æ¢ä¸åŒçš„æ¨¡å‹æä¾›å•†
- ä½œä¸ºç‹¬ç«‹å¾®æœåŠ¡è¿è¡Œ

### è®¾è®¡å“²å­¦
```
å•ä¸€èŒè´£ (SRP)
â”œâ”€ ä¸“æ³¨äº embedding + chat æ¨¡å‹
â”œâ”€ ä¸å¤„ç† KB æ„å»ºã€RAG é€»è¾‘ç­‰
â””â”€ æä¾›é€šç”¨æ¥å£ä¾›å…¶ä»–æœåŠ¡ä½¿ç”¨

æä¾›å•†æ— å…³ (Provider-Agnostic)
â”œâ”€ æ”¯æŒ Ollamaï¼ˆæœ¬åœ°å¼€æºæ¨¡å‹ï¼‰
â”œâ”€ æ”¯æŒ OpenAI-compatibleï¼ˆäº‘æœåŠ¡ï¼‰
â””â”€ æ˜“äºæ‰©å±•æ–°æä¾›å•†

æ˜“äºé›†æˆ (Integration-Ready)
â”œâ”€ åº“æ¨¡å¼ï¼šimport å¯¼å…¥ä½¿ç”¨
â”œâ”€ æœåŠ¡æ¨¡å¼ï¼šç‹¬ç«‹ REST API è¿è¡Œ
â””â”€ é…ç½®é©±åŠ¨ï¼šé€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶
```

---

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

### ç›®å½•å¸ƒå±€
```
embedding-service/
â”œâ”€â”€ embedding_service/          # ä¸»åŒ…
â”‚   â”œâ”€â”€ __init__.py            # å…¬å¼€æ¥å£
â”‚   â”œâ”€â”€ __main__.py            # CLI å…¥å£ (å¯é€‰)
â”‚   â”œâ”€â”€ config.py              # é…ç½®ç®¡ç† (67 è¡Œ)
â”‚   â”œâ”€â”€ embeddings.py          # æ¨¡å‹å·¥å‚ (60 è¡Œ)
â”‚   â””â”€â”€ api.py                 # REST API (110 è¡Œ)
â”‚
â”œâ”€â”€ test_service.py            # å•å…ƒæµ‹è¯• (45 è¡Œ)
â”œâ”€â”€ example.py                 # ä½¿ç”¨ç¤ºä¾‹ (60 è¡Œ)
â”œâ”€â”€ requirements.txt           # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ README.md                  # é¡¹ç›®æ–‡æ¡£
â”œâ”€â”€ Makefile                   # æ„å»ºå·¥å…·
â””â”€â”€ .gitignore                # Git å¿½ç•¥è§„åˆ™
```

### ä»£ç ç»Ÿè®¡
- **æ€»ä»£ç è¡Œæ•°**: ~400 è¡Œ
- **æ ¸å¿ƒåº“ä»£ç **: ~140 è¡Œ (config + embeddings)
- **API ä»£ç **: ~110 è¡Œ
- **æµ‹è¯•ä»£ç **: ~45 è¡Œ

---

## ğŸ”Œ æ ¸å¿ƒæ¨¡å—åˆ†æ

### 1. **config.py** (é…ç½®ç®¡ç†æ¨¡å—)

#### èŒè´£
- é›†ä¸­ç®¡ç†æ‰€æœ‰é…ç½®å‚æ•°
- ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
- æä¾›é…ç½®éªŒè¯å’Œè§„èŒƒåŒ–

#### å…³é”®ç±»: Settings

```python
@dataclass
class Settings:
    provider: str              # "ollama" | "openai-compatible"
    llm_model: str            # èŠå¤©æ¨¡å‹åç§°
    embed_model: str          # åµŒå…¥æ¨¡å‹åç§°
    openai_base_url: str | None    # OpenAI API åœ°å€
    openai_api_key: str | None     # OpenAI API å¯†é’¥
    ollama_base_url: str | None    # Ollama æœåŠ¡åœ°å€
```

#### ç¯å¢ƒå˜é‡æ˜ å°„

| ç¯å¢ƒå˜é‡ | é»˜è®¤å€¼ | å«ä¹‰ |
|---------|--------|------|
| `PROVIDER` | `ollama` | LLM æä¾›å•† |
| `LLM_MODEL` | `qwen2.5:3b` | èŠå¤©æ¨¡å‹ |
| `EMBED_MODEL` | `mxbai-embed-large` | åµŒå…¥æ¨¡å‹ |
| `OLLAMA_BASE_URL` | `http://localhost:11434` | Ollama åœ°å€ |
| `OPENAI_BASE_URL` | `None` | OpenAI API åœ°å€ |
| `OPENAI_API_KEY` | `None` | OpenAI å¯†é’¥ |

#### å·¥å…·å‡½æ•°

```python
def _get_int(name: str, default: int) -> int
    â””â”€ å®‰å…¨è·å–æ•´æ•°ç¯å¢ƒå˜é‡

def _get_float(name: str, default: float) -> float
    â””â”€ å®‰å…¨è·å–æµ®ç‚¹æ•°ç¯å¢ƒå˜é‡

def clamp_provider(provider: str) -> str
    â””â”€ è§„èŒƒåŒ–æä¾›å•†åç§°
    â””â”€ å¯¹æ— æ•ˆå€¼è¿”å› "ollama" (é»˜è®¤)
```

#### é…ç½®åŠ è½½æµç¨‹

```python
# 1. ä»ç¯å¢ƒå˜é‡åˆ›å»ºé…ç½®
settings = Settings.from_env()

# 2. è‡ªåŠ¨è§„èŒƒåŒ–æä¾›å•†åç§°
provider = clamp_provider("OLLAMA")  # â†’ "ollama"

# 3. å›é€€åˆ°é»˜è®¤å€¼
# è‹¥ PROVIDER æœªè®¾ç½® â†’ "ollama"
# è‹¥ EMBED_MODEL æœªè®¾ç½® â†’ "mxbai-embed-large"
```

---

### 2. **embeddings.py** (æ¨¡å‹å·¥å‚æ¨¡å—)

#### èŒè´£
- åˆ›å»ºåµŒå…¥æ¨¡å‹å®ä¾‹
- åˆ›å»ºèŠå¤©æ¨¡å‹å®ä¾‹
- å±è”½æä¾›å•†å·®å¼‚

#### å…³é”®å‡½æ•°: build_embeddings()

```python
def build_embeddings(
    provider: str,
    embed_model: str,
    base_url: str | None = None,
    api_key: str | None = None,
) -> Embeddings
```

**å·¥ä½œæµç¨‹**:
1. è§„èŒƒåŒ– provider åç§°
2. æ ¹æ® provider ç±»å‹é€‰æ‹©å®ç°
3. åˆ›å»ºå¹¶è¿”å›æ¨¡å‹å®ä¾‹

**æ”¯æŒçš„æä¾›å•†**:

| æä¾›å•† | å®ç° | ä¾èµ– | é…ç½® |
|------|------|------|------|
| `ollama` | `OllamaEmbeddings` | `langchain-ollama` | base_url |
| `openai-compatible` | `OpenAIEmbeddings` | `langchain-openai` | base_url + api_key |

**ä½¿ç”¨ç¤ºä¾‹**:

```python
# Ollama (æœ¬åœ°)
embeddings = build_embeddings(
    provider="ollama",
    embed_model="mxbai-embed-large",
    base_url="http://localhost:11434"
)

# OpenAI
embeddings = build_embeddings(
    provider="openai-compatible",
    embed_model="text-embedding-3-large",
    base_url="https://api.openai.com/v1",
    api_key="sk-..."
)

# è°ƒç”¨ API
vector = embeddings.embed_query("Hello world")
vectors = embeddings.embed_documents(["doc1", "doc2"])
```

#### å…³é”®å‡½æ•°: build_chat_model()

```python
def build_chat_model(
    provider: str,
    llm_model: str,
    base_url: str | None = None,
    api_key: str | None = None,
    temperature: float = 0.0,
) -> BaseChatModel
```

**å·¥ä½œæµç¨‹**:
1. è§„èŒƒåŒ– provider
2. æ ¹æ® provider é€‰æ‹©å®ç°
3. åˆ›å»ºèŠå¤©æ¨¡å‹ (temperature å½±å“ç”Ÿæˆçš„éšæœºæ€§)

**æ”¯æŒçš„æä¾›å•†**:

| æä¾›å•† | å®ç° | ä¾èµ– |
|------|------|------|
| `ollama` | `ChatOllama` | `langchain-ollama` |
| `openai-compatible` | `ChatOpenAI` | `langchain-openai` |

**temperature å‚æ•°**:
- `0.0` - ç¡®å®šæ€§ç”Ÿæˆ (ç›¸åŒè¾“å…¥ â†’ ç›¸åŒè¾“å‡º)
- `0.5` - å¹³è¡¡ï¼Œæœ‰è½»å¾®å˜åŒ–
- `1.0` - é«˜éšæœºæ€§ï¼Œåˆ›æ„è¾“å‡º

**ä½¿ç”¨ç¤ºä¾‹**:

```python
from langchain_core.messages import HumanMessage, SystemMessage

chat = build_chat_model(
    provider="ollama",
    llm_model="qwen2.5:3b",
    temperature=0.0
)

messages = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content="What is AI?")
]

response = chat.invoke(messages)
print(response.content)
```

---

### 3. **api.py** (REST API æ¨¡å—)

#### èŒè´£
- æš´éœ² HTTP REST ç«¯ç‚¹
- å¤„ç†è¯·æ±‚/å“åº”åºåˆ—åŒ–
- é”™è¯¯å¤„ç†å’Œæ—¥å¿—

#### æ¶æ„æ¨¡å¼

```
FastAPI App
â”œâ”€â”€ åˆå§‹åŒ–æ—¶
â”‚   â”œâ”€â”€ è¯»å– Settings.from_env()
â”‚   â”œâ”€â”€ åˆ›å»º embeddings å®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ åˆ›å»º chat_model å®¢æˆ·ç«¯
â”‚   â””â”€ æ³¨å†Œè·¯ç”±
â”‚
â””â”€â”€ è¿è¡Œæ—¶
    â””â”€ è¯·æ±‚ â†’ å¤„ç† â†’ å“åº”
```

#### æ•°æ®æ¨¡å‹ (Pydantic)

**è¯·æ±‚æ¨¡å‹**:
```python
class QueryRequest(BaseModel):
    text: str                          # å•ä¸ªæŸ¥è¯¢æ–‡æœ¬

class DocumentsRequest(BaseModel):
    texts: List[str]                   # å¤šä¸ªæ–‡æ¡£

class ChatRequest(BaseModel):
    message: str                       # èŠå¤©æ¶ˆæ¯
```

**å“åº”æ¨¡å‹**:
```python
class EmbeddingResponse(BaseModel):
    embedding: List[float]             # å•ä¸ªå‘é‡ [1024]

class EmbeddingsResponse(BaseModel):
    embeddings: List[List[float]]      # å¤šä¸ªå‘é‡ [[1024], ...]

class ChatResponse(BaseModel):
    response: str                      # èŠå¤©å›å¤

class HealthResponse(BaseModel):
    status: str                        # "ok" | "error"
    provider: str                      # "ollama" | "openai-compatible"
    embed_model: str                   # æ¨¡å‹åç§°
    llm_model: str                     # æ¨¡å‹åç§°
```

#### REST ç«¯ç‚¹

| æ–¹æ³• | ç«¯ç‚¹ | åŠŸèƒ½ | è¯·æ±‚ | å“åº” |
|------|------|------|------|------|
| GET | `/health` | å¥åº·æ£€æŸ¥ | - | HealthResponse |
| POST | `/embed/query` | å•ä¸ªæ–‡æœ¬åµŒå…¥ | QueryRequest | EmbeddingResponse |
| POST | `/embed/documents` | æ‰¹é‡æ–‡æœ¬åµŒå…¥ | DocumentsRequest | EmbeddingsResponse |
| POST | `/chat` | èŠå¤©å¯¹è¯ | ChatRequest | ChatResponse |

#### ç«¯ç‚¹è¯¦è§£

**1. GET /health**
```json
è¯·æ±‚: (æ— )

å“åº” (200 OK):
{
  "status": "ok",
  "provider": "ollama",
  "embed_model": "mxbai-embed-large",
  "llm_model": "qwen2.5:3b"
}
```

**2. POST /embed/query**
```json
è¯·æ±‚:
{
  "text": "Machine learning is the future"
}

å“åº” (200 OK):
{
  "embedding": [0.123, -0.456, ..., 0.789]  # 1024 ç»´å‘é‡
}

å“åº” (500 Error):
{
  "detail": "Model not found"
}
```

**3. POST /embed/documents**
```json
è¯·æ±‚:
{
  "texts": [
    "Document 1",
    "Document 2",
    "Document 3"
  ]
}

å“åº” (200 OK):
{
  "embeddings": [
    [0.123, -0.456, ...],
    [0.789, -0.012, ...],
    [0.345, -0.678, ...]
  ]
}
```

**4. POST /chat**
```json
è¯·æ±‚:
{
  "message": "What is artificial intelligence?"
}

å“åº” (200 OK):
{
  "response": "Artificial intelligence (AI) is the simulation of human intelligence..."
}
```

#### é”™è¯¯å¤„ç†

```python
try:
    # æ‰§è¡Œæ“ä½œ
    vector = embeddings.embed_query(request.text)
except Exception as e:
    # è¿”å› HTTP 500 é”™è¯¯
    raise HTTPException(status_code=500, detail=str(e))
```

**å¸¸è§é”™è¯¯**:
- `ModelNotFound` - æ¨¡å‹ä¸å­˜åœ¨æˆ–æœªä¸‹è½½
- `ConnectionError` - Ollama/API æœåŠ¡ä¸å¯ç”¨
- `InvalidRequest` - è¾“å…¥æ ¼å¼é”™è¯¯
- `RateLimitError` - API é€Ÿç‡é™åˆ¶

---

## ğŸ“¦ ä¾èµ–åˆ†æ

### Python ä¾èµ–

```
langchain-core>=0.3.0           # LLM æ¡†æ¶æ ¸å¿ƒ
  â”œâ”€ BaseLanguageModel          # åŸºç±»
  â”œâ”€ BaseChatModel              # èŠå¤©åŸºç±»
  â”œâ”€ Embeddings                 # åµŒå…¥åŸºç±»
  â””â”€ messages                   # æ¶ˆæ¯ç±»å‹

langchain-ollama>=0.2.0         # Ollama é›†æˆ
  â”œâ”€ OllamaEmbeddings
  â”œâ”€ ChatOllama
  â””â”€ æ”¯æŒæœ¬åœ°å¼€æºæ¨¡å‹

langchain-openai>=0.2.0         # OpenAI é›†æˆ
  â”œâ”€ OpenAIEmbeddings
  â”œâ”€ ChatOpenAI
  â””â”€ æ”¯æŒ OpenAI å’Œå…¼å®¹ API

fastapi>=0.115.0                # Web æ¡†æ¶
  â”œâ”€ å¿«é€Ÿ HTTP æœåŠ¡
  â”œâ”€ è‡ªåŠ¨ OpenAPI æ–‡æ¡£
  â””â”€ å¼‚æ­¥æ”¯æŒ

uvicorn>=0.32.0                 # ASGI æœåŠ¡å™¨
  â””â”€ è¿è¡Œ FastAPI åº”ç”¨

pydantic>=2.9.0                 # æ•°æ®éªŒè¯
  â”œâ”€ æ¨¡å‹å®šä¹‰
  â”œâ”€ ç±»å‹æ£€æŸ¥
  â””â”€ åºåˆ—åŒ–/ååºåˆ—åŒ–
```

### ä¾èµ–æ ‘

```
embedding-service
â”œâ”€â”€ langchain-core (LLM æ¡†æ¶)
â”‚   â””â”€â”€ pydantic (æ•°æ®éªŒè¯)
â”œâ”€â”€ langchain-ollama (æœ¬åœ°æ¨¡å‹)
â”‚   â””â”€â”€ langchain-core
â”œâ”€â”€ langchain-openai (äº‘ API)
â”‚   â””â”€â”€ langchain-core
â”œâ”€â”€ fastapi (Web æ¡†æ¶)
â”‚   â”œâ”€â”€ pydantic
â”‚   â””â”€â”€ starlette
â””â”€â”€ uvicorn (æœåŠ¡å™¨)
    â””â”€â”€ asgi
```

### ç‰ˆæœ¬å…¼å®¹æ€§

| ç»„ä»¶ | æœ€å°ç‰ˆæœ¬ | æ¨èç‰ˆæœ¬ | å¤‡æ³¨ |
|------|---------|---------|------|
| Python | 3.8+ | 3.10+ | langchain éœ€è¦ 3.8+ |
| langchain | 0.2.0 | 0.3+ | é¢‘ç¹æ›´æ–° |
| OpenAI API | - | æœ€æ–° | æ”¯æŒ gpt-4, gpt-3.5 |

---

## ğŸ”„ å·¥ä½œæµç¨‹

### åœºæ™¯ 1: ä½œä¸ºåº“ä½¿ç”¨ (Library Mode)

```python
# 1. å¯¼å…¥
from embedding_service import build_embeddings, build_chat_model

# 2. åˆ›å»ºæ¨¡å‹
embeddings = build_embeddings(
    provider="ollama",
    embed_model="mxbai-embed-large"
)

chat = build_chat_model(
    provider="ollama",
    llm_model="qwen2.5:3b"
)

# 3. ä½¿ç”¨
vector = embeddings.embed_query("Hello")
response = chat.invoke([...])

# 4. é›†æˆåˆ°å…¶ä»–åº”ç”¨
# kb_builder / rag_service ç­‰éƒ½å¯ä»¥ä½¿ç”¨
```

**ä¼˜ç‚¹**:
- âœ… è½»é‡çº§ï¼Œæ— å¯åŠ¨å¼€é”€
- âœ… å¯ç›´æ¥é›†æˆåˆ°å…¶ä»–åº”ç”¨
- âœ… å®Œå…¨æ§åˆ¶ç”Ÿå‘½å‘¨æœŸ

### åœºæ™¯ 2: ä½œä¸ºå¾®æœåŠ¡è¿è¡Œ (Microservice Mode)

```bash
# 1. å¯åŠ¨æœåŠ¡
export PROVIDER=ollama
export EMBED_MODEL=mxbai-embed-large
export LLM_MODEL=qwen2.5:3b
uvicorn embedding_service.api:app --host 0.0.0.0 --port 8000

# 2. è¿œç¨‹è°ƒç”¨
curl -X POST http://localhost:8000/embed/query \
  -H "Content-Type: application/json" \
  -d '{"text": "Hello"}'

# 3. å¤šä¸ªåº”ç”¨å¯å…±äº«ä¸€ä¸ª embedding-service å®ä¾‹
```

**ä¼˜ç‚¹**:
- âœ… ç‹¬ç«‹éƒ¨ç½²å’Œæ‰©å±•
- âœ… å¤šä¸ªåº”ç”¨å…±äº«æ¨¡å‹
- âœ… ç½‘ç»œéš”ç¦»
- âœ… å®¹å™¨åŒ–éƒ¨ç½²

### åœºæ™¯ 3: æä¾›å•†åˆ‡æ¢

```python
# åœºæ™¯ 3a: Ollama â†’ OpenAI
embeddings = build_embeddings(
    provider="openai-compatible",
    embed_model="text-embedding-3-large",
    base_url="https://api.openai.com/v1",
    api_key="sk-..."
)

# ä»£ç æ— éœ€æ”¹åŠ¨ï¼Œåªéœ€æ”¹é…ç½®ï¼
# é€šè¿‡ docker-compose æˆ– k8s é…ç½®ç®¡ç†
```

---

## ğŸ§ª æµ‹è¯•åˆ†æ

### æµ‹è¯•è¦†ç›–

```python
# test_service.py - 45 è¡Œ

class TestConfig(unittest.TestCase):
    def test_clamp_provider(self)
        # éªŒè¯æä¾›å•†åç§°è§„èŒƒåŒ–
        âœ“ "ollama" â†’ "ollama"
        âœ“ "OLLAMA" â†’ "ollama"
        âœ“ "openai-compatible" â†’ "openai-compatible"
        âœ“ "invalid" â†’ "ollama" (é»˜è®¤)
    
    def test_build_embeddings_ollama(self)
        # Mock æµ‹è¯• Ollama åµŒå…¥åˆ›å»º
        âœ“ æ­£ç¡®ä¼ é€’å‚æ•°
        âœ“ è¿”å› OllamaEmbeddings å®ä¾‹
```

### æµ‹è¯•ç­–ç•¥

**å•å…ƒæµ‹è¯•** (å·²å®ç°):
- âœ… é…ç½®è§„èŒƒåŒ–
- âœ… æ¨¡å‹å·¥å‚åˆ›å»º
- âš ï¸ éœ€è¦ Mock é¿å…å®é™…ç½‘ç»œè°ƒç”¨

**é›†æˆæµ‹è¯•** (éœ€è¦):
- âŒ çœŸå® Ollama è¿æ¥
- âŒ çœŸå® OpenAI API è°ƒç”¨
- âŒ API ç«¯ç‚¹å®Œæ•´æµç¨‹

**æµ‹è¯•è¿è¡Œ**:
```bash
# è¿è¡Œå•å…ƒæµ‹è¯•
python -m unittest test_service.py -v

# è¾“å‡º
test_clamp_provider ... ok
test_build_embeddings_ollama ... ok
Ran 2 tests in 0.357s
OK
```

---

## ğŸ’¡ è®¾è®¡æ¨¡å¼

### 1. å·¥å‚æ¨¡å¼ (Factory Pattern)

```python
# embeddings.py ä¸­çš„ä¸¤ä¸ªå·¥å‚å‡½æ•°
def build_embeddings(...) -> Embeddings
    # æ ¹æ® provider å‚æ•°åˆ›å»ºåˆé€‚çš„å®ä¾‹
    
def build_chat_model(...) -> BaseChatModel
    # åŒä¸Šï¼Œåˆ›å»ºèŠå¤©æ¨¡å‹
```

**ä¼˜ç‚¹**:
- éšè—å®ç°ç»†èŠ‚
- æ˜“äºæ‰©å±•æ–°æä¾›å•†
- å®¢æˆ·ç«¯ä»£ç ä¸éœ€è¦æ”¹å˜

### 2. é…ç½®å¯¹è±¡æ¨¡å¼ (Configuration Object)

```python
@dataclass
class Settings:
    # é›†ä¸­æ‰€æœ‰é…ç½®
    # ä»ç¯å¢ƒå˜é‡åŠ è½½
    # æä¾›é»˜è®¤å€¼
```

**ä¼˜ç‚¹**:
- å•ä¸€ä¿¡æ¯æ¥æº
- æ˜“äºéªŒè¯å’Œè§„èŒƒåŒ–
- ä¾¿äºä¼ é€’å’Œå…±äº«

### 3. ä¾èµ–æ³¨å…¥ (Dependency Injection)

```python
# FastAPI åº”ç”¨åˆå§‹åŒ–æ—¶æ³¨å…¥ä¾èµ–
def create_app() -> FastAPI:
    settings = Settings.from_env()
    embeddings = build_embeddings(...)
    chat_model = build_chat_model(...)
    # åœ¨è·¯ç”±ä¸­ä½¿ç”¨æ³¨å…¥çš„å®ä¾‹
```

**ä¼˜ç‚¹**:
- ä¾¿äºæµ‹è¯• (Mock æ³¨å…¥)
- è§£è€¦ç»„ä»¶
- ç”Ÿå‘½å‘¨æœŸç®¡ç†

---

## ğŸš€ ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1: RAG ç³»ç»Ÿä¸­çš„åµŒå…¥ç»„ä»¶

```python
# kb_builder.py ä¸­ä½¿ç”¨
from embedding_service import build_embeddings

embeddings = build_embeddings(
    provider=settings.provider,
    embed_model=settings.embed_model,
    base_url=settings.ollama_base_url
)

# ä¸ºçŸ¥è¯†åº“å—ç”Ÿæˆå‘é‡
vectors = embeddings.embed_documents(chunks)
# ä¿å­˜åˆ° FAISS ç´¢å¼•
```

### åœºæ™¯ 2: RAG ç³»ç»Ÿä¸­çš„æ£€ç´¢ç»„ä»¶

```python
# rag.py ä¸­ä½¿ç”¨
from embedding_service import build_embeddings

embeddings = build_embeddings(...)

# æŸ¥è¯¢å‘é‡åŒ–
query_vec = embeddings.embed_query(question)

# FAISS æ£€ç´¢
scores, indices = kb.index.search(query_vec, k=5)
```

### åœºæ™¯ 3: ç‹¬ç«‹èŠå¤©æœåŠ¡

```bash
# å¯åŠ¨ embedding-service ä½œä¸ºèŠå¤©å¾®æœåŠ¡
$ PROVIDER=openai uvicorn embedding_service.api:app

# å…¶ä»–åº”ç”¨è°ƒç”¨
$ curl -X POST http://embedding-service:8000/chat \
    -d '{"message": "Hello"}'
```

### åœºæ™¯ 4: å¤šæ¨¡å‹éƒ¨ç½²

```
â”Œâ”€ Embedding Service (mxbai-embed-large)
â”‚  â””â”€ KB Builder ä½¿ç”¨
â”‚
â”œâ”€ Chat Service (qwen2.5:3b)
â”‚  â””â”€ RAG/API ä½¿ç”¨
â”‚
â””â”€ Vision Service (llava)
   â””â”€ å›¾æ–‡ç†è§£ä½¿ç”¨

æ¯ä¸ªæœåŠ¡ç‹¬ç«‹éƒ¨ç½²ï¼Œäº’ä¸å½±å“
```

---

## âš™ï¸ é…ç½®æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: ç¯å¢ƒå˜é‡ (å¼€å‘ç¯å¢ƒ)

```bash
# .env æˆ– shell
export PROVIDER=ollama
export EMBED_MODEL=mxbai-embed-large
export LLM_MODEL=qwen2.5:3b
export OLLAMA_BASE_URL=http://localhost:11434

# å¯åŠ¨
python -m embedding_service
```

### æ–¹æ¡ˆ 2: Docker ç¯å¢ƒå˜é‡ (å®¹å™¨åŒ–)

```dockerfile
FROM python:3.11
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY embedding_service ./embedding_service
ENV PROVIDER=ollama
ENV EMBED_MODEL=mxbai-embed-large
ENV OLLAMA_BASE_URL=http://ollama:11434
CMD ["uvicorn", "embedding_service.api:app", "--host", "0.0.0.0"]
```

### æ–¹æ¡ˆ 3: Docker Compose æœåŠ¡ç¼–æ’

```yaml
version: '3.8'
services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
  
  embedding-service:
    build: .
    ports:
      - "8000:8000"
    environment:
      PROVIDER: ollama
      OLLAMA_BASE_URL: http://ollama:11434
    depends_on:
      - ollama
```

---

## ğŸ“Š æ€§èƒ½ç‰¹æ€§

### åµŒå…¥æ€§èƒ½

| æ“ä½œ | æ¨¡å‹ | å»¶è¿Ÿ | ååé‡ |
|------|------|------|--------|
| å•æ–‡æœ¬åµŒå…¥ | mxbai-embed-large | ~100ms | - |
| æ‰¹é‡ (100 æ–‡æœ¬) | mxbai-embed-large | ~500ms | 200 docs/sec |
| OpenAI API | text-embedding-3-large | ~100ms | å— API é™æµ |

### èŠå¤©æ€§èƒ½

| æ“ä½œ | æ¨¡å‹ | å»¶è¿Ÿ | å¤‡æ³¨ |
|------|------|------|------|
| çŸ­å›å¤ (<50 tokens) | qwen2.5:3b | 1-2 sec | æœ¬åœ° Ollama |
| é•¿å›å¤ (>500 tokens) | qwen2.5:3b | 5-10 sec | æœ¬åœ° Ollama |
| OpenAI API | gpt-4 | 1-3 sec | äº‘æœåŠ¡ |

### èµ„æºä½¿ç”¨

| èµ„æº | Ollama æœ¬åœ° | OpenAI API |
|------|-----------|-----------|
| å†…å­˜ | 4-8 GB | ~100 MB |
| GPU | éœ€è¦ | ä¸éœ€è¦ |
| ç½‘ç»œ | æœ¬åœ° | éœ€è¦ |
| æˆæœ¬ | 0 (ç¡¬ä»¶æˆæœ¬) | æŒ‰ç”¨é‡è®¡è´¹ |

---

## ğŸ”’ å®‰å…¨è€ƒè™‘

### API Key ç®¡ç†

```python
# âœ… å¥½çš„åšæ³•ï¼šç¯å¢ƒå˜é‡
OPENAI_API_KEY=sk-... (åœ¨ .env æˆ– k8s secret)

# âŒ åçš„åšæ³•ï¼šç¡¬ç¼–ç 
api_key = "sk-..."  # ä¸è¦è¿™æ ·åšï¼
```

### è¾“å…¥éªŒè¯

```python
# Pydantic è‡ªåŠ¨éªŒè¯
class QueryRequest(BaseModel):
    text: str  # å¿…é¡»æ˜¯å­—ç¬¦ä¸²

# è‹¥æäº¤æ•°æ®ç±»å‹é”™è¯¯ï¼ŒFastAPI è‡ªåŠ¨æ‹’ç»
```

### é”™è¯¯ä¿¡æ¯

```python
# âœ… å®‰å…¨ï¼šéšè—å†…éƒ¨ç»†èŠ‚
raise HTTPException(status_code=500, detail="Internal error")

# âŒ ä¸å®‰å…¨ï¼šæš´éœ²å †æ ˆè·Ÿè¸ª
detail=traceback.format_exc()
```

### é€Ÿç‡é™åˆ¶

```python
# ç”Ÿäº§ç¯å¢ƒå»ºè®®æ·»åŠ 
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

@limiter.limit("100/minute")
@app.post("/chat")
def chat(...): ...
```

---

## ğŸ”„ ä¸å…¶ä»–æ¨¡å—çš„é›†æˆ

### ä¸ docx-parser çš„å…³ç³»

```
docx-parser
  â””â”€ è§£ææ–‡æ¡£ä¸ºæ–‡æœ¬å—
     â””â”€ embedding-service
        â””â”€ ä¸ºæ–‡æœ¬å—ç”Ÿæˆå‘é‡
           â””â”€ kb-builder
              â””â”€ æ„å»ºçŸ¥è¯†åº“ç´¢å¼•
```

### ä¸ kb-builder çš„å…³ç³»

```
kb-builder
  â”œâ”€ æ‰«ææ–‡æ¡£
  â”œâ”€ è°ƒç”¨ docx-parser è§£æ
  â”œâ”€ è°ƒç”¨ embedding-service åµŒå…¥å‘é‡
  â””â”€ æ„å»º FAISS ç´¢å¼•
```

### ä¸ rag-service çš„å…³ç³»

```
rag-service
  â”œâ”€ æ¥æ”¶ç”¨æˆ·é—®é¢˜
  â”œâ”€ è°ƒç”¨ embedding-service å‘é‡åŒ–æŸ¥è¯¢
  â”œâ”€ FAISS æ£€ç´¢ç›¸å…³å—
  â”œâ”€ è°ƒç”¨ embedding-service èŠå¤©æ¨¡å‹
  â””â”€ ç”Ÿæˆå›ç­”
```

### ä¸ customer-service-api çš„å…³ç³»

```
customer-service-api
  â””â”€ é›†æˆ embedding-service (æœ¬åœ°æˆ–è¿œç¨‹)
     â””â”€ ç”¨äºçŸ¥è¯†åº“æŸ¥è¯¢å’ŒèŠå¤©
```

---

## ğŸ“ˆ æ‰©å±•æ–¹å‘

### 1. æ”¯æŒæ–°æä¾›å•†

```python
# æ·»åŠ  Anthropic Claude æ”¯æŒ
if provider == "anthropic":
    from langchain_anthropic import ChatAnthropic
    return ChatAnthropic(model=llm_model, api_key=api_key)

# æ·»åŠ æœ¬åœ° LLaMA æ”¯æŒ
if provider == "llama-cpp":
    from langchain_community.llms import LlamaCpp
    return LlamaCpp(model_path=model_path, ...)
```

### 2. ç¼“å­˜æ”¯æŒ

```python
# ç¼“å­˜åµŒå…¥ç»“æœï¼Œé¿å…é‡å¤è®¡ç®—
@cache
def embed_query(text: str) -> List[float]:
    return embeddings.embed_query(text)
```

### 3. æ‰¹å¤„ç†ä¼˜åŒ–

```python
# å®ç°æµå¼åµŒå…¥ï¼Œå¤„ç†è¶…å¤§æ–‡æ¡£
def embed_documents_stream(texts: Iterator[str]) -> Iterator[List[float]]:
    for batch in iter_batches(texts, batch_size=100):
        yield embeddings.embed_documents(batch)
```

### 4. ç›‘æ§æŒ‡æ ‡

```python
# æ·»åŠ  Prometheus metrics
embedding_requests_total = Counter(...)
embedding_latency = Histogram(...)
embedding_errors_total = Counter(...)
```

---

## ğŸ“ å­¦ä¹ èµ„æº

### LangChain æ–‡æ¡£
- Embeddings: https://python.langchain.com/docs/integrations/text_embedding/
- Chat Models: https://python.langchain.com/docs/integrations/chat/

### Ollama
- å®˜ç½‘: https://ollama.ai/
- æ¨¡å‹åº“: https://ollama.ai/library
- æœ¬åœ°éƒ¨ç½²: https://github.com/jmorganca/ollama

### OpenAI API
- å®˜ç½‘: https://platform.openai.com/docs
- åµŒå…¥æ¨¡å‹: https://platform.openai.com/docs/guides/embeddings
- èŠå¤©æ¨¡å‹: https://platform.openai.com/docs/guides/gpt

### FastAPI
- å®˜ç½‘: https://fastapi.tiangolo.com/
- éƒ¨ç½²: https://fastapi.tiangolo.com/deployment/

---

## ğŸ† é¡¹ç›®ä¼˜åŠ¿

âœ… **è½»é‡çº§** - ä»… ~300 è¡Œä»£ç ï¼Œæ ¸å¿ƒåŠŸèƒ½å®Œæ•´  
âœ… **æä¾›å•†æ— å…³** - è½»æ¾åˆ‡æ¢ Ollama / OpenAI / å…¶ä»–  
âœ… **åŒæ¨¡å¼** - åº“æ¨¡å¼ + æœåŠ¡æ¨¡å¼ï¼Œçµæ´»ä½¿ç”¨  
âœ… **æ˜“äºæµ‹è¯•** - ä¾èµ–æ³¨å…¥ï¼Œä¾¿äº Mock  
âœ… **é…ç½®é©±åŠ¨** - é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶ï¼Œå®¹å™¨å‹å¥½  
âœ… **ç±»å‹å®‰å…¨** - Pydantic è‡ªåŠ¨éªŒè¯å’Œåºåˆ—åŒ–  
âœ… **è‡ªåŠ¨æ–‡æ¡£** - FastAPI è‡ªåŠ¨ç”Ÿæˆ Swagger UI  

---

## âš ï¸ å·²çŸ¥é™åˆ¶

âŒ **æ— ç¼“å­˜** - ç›¸åŒæ–‡æœ¬é‡å¤åµŒå…¥ä¼šé‡æ–°è®¡ç®—  
âŒ **æ— é‡è¯•é€»è¾‘** - ç½‘ç»œé”™è¯¯ä¼šç›´æ¥å¤±è´¥  
âŒ **æ— é€Ÿç‡é™åˆ¶** - ç”Ÿäº§ç¯å¢ƒéœ€è¦è‡ªå·±æ·»åŠ   
âŒ **æ— å¹¶å‘æ§åˆ¶** - å¯èƒ½åŒæ—¶å¤šä¸ªè¯·æ±‚ç«äº‰  
âŒ **æ¨¡å‹ç¡¬ç¼–ç ** - ä¸æ”¯æŒåŠ¨æ€åˆ‡æ¢æ¨¡å‹  
âŒ **æ— æ—¥å¿—** - ç¼ºå°‘ç»“æ„åŒ–æ—¥å¿—è¾“å‡º  

---

## ğŸ“ ä½¿ç”¨å»ºè®®

1. **å¼€å‘ç¯å¢ƒ**: ä½¿ç”¨ Ollama (æœ¬åœ°ã€å…è´¹ã€å¿«é€Ÿè¿­ä»£)
2. **ç”Ÿäº§ç¯å¢ƒ**: è€ƒè™‘ OpenAI (ç¨³å®šã€æˆç†Ÿã€ä»˜è´¹)
3. **æ··åˆæ–¹æ¡ˆ**: Ollama å¤‡ä»½ + OpenAI ä¸»åŠ›
4. **å®¹å™¨éƒ¨ç½²**: Docker + docker-compose æˆ– Kubernetes
5. **ç›‘æ§å‘Šè­¦**: æ·»åŠ æ—¥å¿—ã€æŒ‡æ ‡ã€å¥åº·æ£€æŸ¥

---

## ğŸš€ ä¸‹ä¸€æ­¥è®¡åˆ’

1. âœ… å®Œæˆ embedding-service (å½“å‰)
2. â³ åˆ›å»º kb-builder é¡¹ç›® (ä¸‹ä¸€æ­¥)
3. â³ åˆ›å»º rag-service é¡¹ç›®
4. â³ åˆ›å»º customer-service-api é¡¹ç›®
5. â³ åˆ†ç¦» customer-service-web é¡¹ç›®

